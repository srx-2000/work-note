# 卷积

## 对卷积的理解

这里放一个知乎上的回答，我感觉对卷积的说明还是很形象，很感性的。

```markdown
作者：知乎用户
链接：https://www.zhihu.com/question/430129801/answer/1886809110
来源：知乎
著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。

	问题一：学习到特征，其是这个说法不准确，反向传播是一个更新参数的过程，就是通过loss来计算预测值和真实值间的差距，然后根据这个差距，利用链式法则不断的求偏微分，然后更新模型中的每一个参数。其实更高级的认识，在CNN中梯度下降来更新卷积核参数的过程就是一个反卷积的过程。卷积核的作用是提取图像的特征，然而一个卷积核是不够的，因为一个卷积核只能反应图像的某一个特征，所以我们需要多个卷积核，这些不同的卷积核可以提取到图像不同的特征，从而让我们的模型学习图像特征的能力更强。这也就是越深的CNN越精确的原因，因为有足够的卷积核和足够的参数可以表述原始图像的特征。
	问题二：一张图像的像素是固定的，变得是有很多图像，不同的图像不断地在这个CNN中训练，导致卷积核的参数不断的通过反向传播更新。最后上亿个参数在N此迭代后，彼此都达到了一个可以接受的值，衡量这个接受的程度就是精度，那么模型就训练好了。所以有一个概念很重要，就是参数初始化，因为好的初始化会让模型训练收敛更快，所以不断有文献对于模型参数的初始化提出不同的解决方案。有兴趣可以看一下。顺便补充一个，所谓特征，很多人其实都不理解，实际上你可以理解为图像的某些部分，比如一个卷积核扫过去后（将一个三层的RGB图像变成一个一层的图像），在经过激活函数后，就是对扫过去新的这个图像上每一个像素值做激活函数，这张新的图像中的山的轮廓被凸显了出来，但是除了这个，你也看不到别的东西了，而另一个卷积核扫过去后，水的轮廓被凸显了出来，所以，很直观的，如果你想要知道这张图到底画的是什么，那是不是就需要很多个特征图加在一起，你才能看到原来图像是桂林山水，所以我们就需要很多的卷积核，就是这个意思。
	再补充一个，如果你不用卷积，那么原始图像比如1,024乘以768乘以3，是2,359,296个像素值，下一层的神经元比如是10的5次方个，你想200多万乘以10的5次方是多少个值，因此，卷积神经网络的核心思想是，降维且不损失，即一方面抽象化原始图像，另一方面利用不同的卷积核提取图像不同的特征以保证最大程度逼近原始图像。当你还原每一个卷积后的图像时，越到后面，越是看不出来原始图像，因为已经高度提取了图像的特征，只有计算机可以看出来是什么。这是一个从具体到抽象的过程。换句话说，也就是一个归纳和演绎的辩证过程。
	还有就是可能你会关心卷积过程中参数的数目，比如一个3乘3的卷积核，在扫过一个三通道的图像时，参数是27个，那么如果这样的卷积核有192个，那么就是27乘以192，即5184个参数。还有一个建议，就是多看英文的东西，中文的东西容易把很多概念混淆。如果有说得不对或者有其他问题，欢迎指正和探讨。
```

​	其中有一个对于我自己很关键的理解：在一个图片上会有很多的卷积核，而不是在平时搜到的下面这个经典的卷积图，或者可以说下面这个图的效果，在一个图片上会有几百个乃至上千或更多的个。一个图片会被一堆卷积核卷，并且这些卷了的之后的卷积核会成为下一个卷积层卷的目标，会有一批新的卷积核接着卷上一层卷出来的东西，从而一层层的将图片的信息在保证不缺失的情况下缩放，抽象。从而抽出图形的特征。

![1685427083529](./卷积.assets/1685427083529.jpg)

补充：上面说到的一个图片需要有很多的卷积核来抽，然后将抽出来的东西进一步抽，整体流程没问题，但是有明显的误导性，这里指正一下。~~首先一定要知道的是卷积核的数量是作为一个超参数人为设置在配置文件中的【**存疑**】~~，所以其在一个模型中大概率是固定的，而且这个固定是以层为单位的。举个例子：这里我设置卷积核的数量是10个，那么这里的意思就是每一层卷积都会有10个卷积核在这层上进行卷积。到下一层同样有10个卷积核进行卷积【如果有下取样或上取样之类的需求，是通过控制每层的卷积核的大小，以及padding，stride来控制的】。

### 真正使用的卷积一层中各个维度应该是什么样子

这里给一个真正代码对应的几何例子：

首先是代码：

```python
import torch
 
in_channels = 5  #输入通道数量
out_channels =10 #输出通道数量
width = 100      #每个输入通道上的卷积尺寸的宽
heigth = 100     #每个输入通道上的卷积尺寸的高
kernel_size = 3  #每个输入通道上的卷积尺寸
batch_size = 1   #批数量
 
input = torch.randn(batch_size,in_channels,width,heigth)
conv_layer = torch.nn.Conv2d(in_channels,out_channels,kernel_size=kernel_size)
 
out_put = conv_layer(input)
 
print(input.shape)
print(out_put.shape)
print(conv_layer.weight.shape)

"""
torch.Size([1, 5, 100, 100])
torch.Size([1, 10, 98, 98])
torch.Size([10, 5, 3, 3])
"""
```

这里主要说一下对应的三个输出：

1. 首先是第一个：[1,5,100,100]，这个也就是平时最开始的输入
   1. 1代表batch_size，一般为16，用以代指输入图片的数量。
   2. 5就代表了卷积层接收到的输入的基础维度，这里猜测可能代指的实际意义是：抽取图片的特增数量，这里就代指抽取图片的5种特征
   3. 后两个100，主要代表了输入图片的大小，分别代表长和宽所包含的像素个数。
2. 其次是第二个：[1,20,98,98]，这个也就是通过一次卷积层之后原来输入的图片卷积后的结果。可以看到在特征维度上变成了10，而在长宽像素上也被卷小了2点。
   1. 1依旧代表了batch_size，代指输出图片的数量
   2. 10就代表了这个卷积层最终输出特征的维度，就如同上面那个5一样，这个10代表的就是经过这层卷积之后，在这些图片中抽出了10个向量特征。
   3. 后两个98，主要代表了经过一层卷积之后，得到的图片的长宽所包含的像素的个数。
3. 最后是：[10,5,3,3]，这个是最关键的一个输出，因为他代表的是经过上面这样的输入之后，这层卷积层所有的参数权重的形状。
   1. 这里的10代表的就是上面所说的输出的特征的维度，对应了下面这个图中10个小容器。
   2. 而5则代表了上面所说的输入的特征的维度，对应了下面图中每个容器中装了的5片featuremap
   3. 最后的两个3，则代表了卷积层中的卷积核的大小

**小总结：**经过上面三个部分的说明，可以想象一下。如果说就有一个网络就是简单的一层卷积，参数也都与上面的一样，那么开始的输入、最终的输出、得到的权重的感性认知应该分别是：

​	**最开始的输入**：我拿到了1张100*100像素的图片，并给出了5个特征

​	**最终的输出**：通过一层卷积的映射之后，卷积网络将上述我给出的这张图片的大小从100\*100的像素，映射到了98\*98像素的一个特征图上，并且将之前的5个特征，扩展为了10个特征。

​	**得到的权重**：通过上面这一次输入及输出的过程，我们的卷积网络学到了如何使用一个3\*3的卷积核，将5个特征映射成为10个特征。而从单位上来讲，3\*3的卷积核就是下图中的一个面，其中的每一个格子都是不同的参数，而5个特征则是5个形状一样，但每个面装的参数都不同的这样的面，从而形成一个**体**一样的东西。最后10个特征则是将上述这样的**体**过了十遍之后，再将每一次不同的参数堆积在一起。



![未命名文件 (1)](./卷积.assets/未命名文件 (1).png)

### 一张图解释明白卷积核的个数以及通道个数之间的关系：

这张图还没有磨出来，但是可以给一下主要想法来源，首先是下面这个文章本身，然后是这个文章下面ambition的评论。

现在具体没有搞清楚的问题有四个：

1. 对于一个大图片来说，理应有多个卷积核在上面进行卷，但是卷的方式是什么：
   1. 是将大图片分割分区，然后每个卷积卷其中一部分，最后将所有的卷积都concat一下
   2. 还是将一个卷积核整体在这一个大图片上卷过一圈之后，再安排下一个卷积核重新卷一遍，得到不同的参数，最后将这些参数相加
2. 参数到底存在哪，是存在在卷积核中么，如果是那么每次卷积出来的那个output又有什么作用呢？是过度给下一层卷积接着卷么？
3. RGB卷积真的是利用三个初始参数不同的卷积核，分别训练，然后再在最后统一加在一起么。
4. 卷积核的数量到底是在哪来设置的。

https://zhuanlan.zhihu.com/p/251068800



这个[链接](https://zhuanlan.zhihu.com/p/95573093)回答了上面部分问题：

**Q**：卷的方式是具体那种？

**A**：应该是第二种方式进行卷的，

**Q**：参数到底存在哪了？

**A**：就是存在卷积核里了，因为会采用上面第二种方式进行卷，所以会有很多个卷积核重复卷同一个图的全部。而参数的个数就是卷积核中的参数的个数的和。

**Q**：每次卷积出来的那个output又有什么作用呢？是过度给下一层卷积接着卷么？

**A**：每次卷出来的output就是为了给下一层卷积接着卷的，所以每经过一层卷积，output就会提取出更抽象的信息

**Q**：RGB卷积真的是利用三个初始参数不同的卷积核，分别训练，然后再在最后统一加在一起么？

**A**：是也不是，具体样子看下图：可以看到是直接用三个卷积核去卷三个channel，但是具体这三个卷积核的初始化参数是否一致并不知道【还需要进一步讨论一下】。

![img](https://img-blog.csdn.net/20180404113714719?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvc3NjY19sZWFybmluZw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

**Q**：卷积核的数量到底是怎么来的？

**A**：没说【继续查】



**小总结**：所以如果按照上面这个链接里的内容来看的话，那么[这里](#真正使用的卷积一层中各个维度应该是什么样子)下面给出的图大致意思就是对的了，同时不难发现之前一直在困扰的channel的数量竟然是图中的5那个东西。也就是说上面那个例子中竟然有5个channel。这个是我没想到的，也是上面表达错误的地方，亦或是现在的理解才是错误的【这里提出一个新的问题，详见下面新问题】，还需要进一步的讨论。

**新问题：**如果按照现在的理解来看的话，那么上面的例子后续其将channel映射到了10，是不是就意味着他选择了10个channel来提取特征，那么此时就出现了一个问题，图像的特征的数量是与channel数量挂钩的么？如果是的话那岂不是说如果后面接入第二个卷积层时，他的第二层channel就扩展到了10？这与日常理解的图片是由RGB三个channel组成的结论相差太大了。