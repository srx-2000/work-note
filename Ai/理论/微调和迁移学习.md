# 微调和迁移学习

## 二者的联系和区别

### 第一版理解

**联系**

​	二者都是拿到别人在大数据集下训练好的大模型的一系列权重参数作为自己模型的最开始的部分

**区别**

1. 虽然都是拿大模型训练好的模型参数，但不同的是迁移学习时直接将拿到的这些模型参数，利用原模型架构加载出来之后，再在后面直接接入一个线性层【应该也可以是其他更复杂的结构】，将原模型部分的参数冻结住，并用这些冻结住的参数作为后面接入部分的输入，直接进行预测，本身并不具有对原模型参数权重进行调整这一步。
2. 而微调则是在拿到原模型的参数权重之后，同样是利用原模型架构加载出来，并在后面接入一个网络。但是不同的是微调会先将原模型的权重参数冻结，使其作为后续模型的初始输入，随后利用上述得到的初始输入对后续接入的模型**进行训练**【这是第一个与迁移学习不同的点，他是真的用上面的输入训练了后续的模型的，而不是像迁移学习一样直接进行预测】，并将这个接入的后续模型的loss训练到一定的程度【足够低】。而后将之前冻结的原模型的参数解冻，并与刚刚训练出来的模型参数做一个参数共享，使后续训练的参数可以反过来影响上面的原模型中的参数权重，在进行进一步的训练，直到模型最终成型。

**总结**

​	换句话说，如果模型微调如果没有后面的训练部分，那本质上就是迁移学习。